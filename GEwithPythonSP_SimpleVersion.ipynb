{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76ed59e1-ef9e-4053-88f9-2fa17b72168a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 7, 0)\n",
      "[Row(CURRENT_WAREHOUSE()='CLUSTER1', CURRENT_DATABASE()='NYCTAXI', CURRENT_SCHEMA()='TAXI')]\n"
     ]
    }
   ],
   "source": [
    "from snowflake.snowpark.session import Session\n",
    "from snowflake.snowpark.functions import udf, avg, col,lit,call_udf,countDistinct,sproc,udf\n",
    "from snowflake.snowpark.types import IntegerType, FloatType, StringType, BooleanType\n",
    "import pandas as pd\n",
    "from config import snowflake_conn_prop_local as snowflake_conn_prop\n",
    "import sys\n",
    "import json\n",
    "import platform\n",
    "import os,requests\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "\n",
    "from snowflake.snowpark import version\n",
    "print(version.VERSION)\n",
    "session = Session.builder.configs(snowflake_conn_prop).create()\n",
    "\n",
    "print(session.sql('select current_warehouse(), current_database(), current_schema()').collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc13ace2-9ede-47db-bcce-42ec5eb55ea3",
   "metadata": {},
   "source": [
    "### Initialize\n",
    "\n",
    "- Here we are downloading the GE 15.14 version from the Pypi to theb local folder on the machine where this code is running.\n",
    "- This has been tested with GE 15.14. There was some issue with 15.17 where its trying to look for the package ipywidgets which is found in Snowflake anaconda channel but not found in information_schema.packages \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59d618b1-3881-4f5d-8fb8-59e7a4c7483b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./temp/tarfiles/great_expectations-0.15.14.tar.gz\n",
      "Create local dir: ./temp/tarfiles\n",
      "Create local dir: ./temp/libs\n",
      "Downloading library from PyPI to ./temp/tarfiles ...\n",
      "Started Extracting GE tar file to ./temp/tarfiles ...\n",
      "Done extracting GE tar file to ./temp/tarfiles ...\n"
     ]
    }
   ],
   "source": [
    "# Current working directory is the Project Home DIR.\n",
    "PROJECT_HOME_DIR = '.'\n",
    "LOCAL_TEMP_DIR = os.path.join(PROJECT_HOME_DIR, 'temp') \n",
    "LOCAL_LIB_DIR = os.path.join(LOCAL_TEMP_DIR, 'libs')\n",
    "LOCAL_TARFile_DIR = os.path.join(LOCAL_TEMP_DIR, 'tarfiles')\n",
    "LIB_URLS = [\n",
    "    #'https://files.pythonhosted.org/packages/9f/57/1539d783553f3d67cea1b55d7fe494373c5c0c9af689d4c0e0c2d3197739/great_expectations-0.15.17-py3-none-any.whl'\n",
    "    'https://files.pythonhosted.org/packages/8e/9d/cecb12289f7967b15facf550a0bbb9c1e910968c3a61b91fd8cdb80aeb3c/great_expectations-0.15.14.tar.gz'\n",
    "    \n",
    "]\n",
    "\n",
    "for lib_url in LIB_URLS:\n",
    "    # get the file name, from the url\n",
    "    splits = lib_url.split('/')\n",
    "    tot_splits = len(splits)\n",
    "    target_file = splits[-1]\n",
    "    \n",
    "    local_lib_fl = f'{LOCAL_TARFile_DIR}/{target_file}'\n",
    "    print(local_lib_fl)\n",
    "\n",
    "    # Create a local directory for TAR and extracting tar..\n",
    "    Path(LOCAL_TARFile_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    print(f'Create local dir: {LOCAL_TARFile_DIR}')\n",
    "\n",
    "    Path(LOCAL_LIB_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    print(f'Create local dir: {LOCAL_LIB_DIR}')\n",
    "\n",
    "    print(f'Downloading library from PyPI to {LOCAL_TARFile_DIR} ...')\n",
    "    with open(local_lib_fl, \"wb\") as f:\n",
    "        r = requests.get(lib_url)\n",
    "        f.write(r.content)\n",
    "\n",
    "        \n",
    "# Extract GE tar file\n",
    "\n",
    "import tarfile\n",
    "file = tarfile.open(local_lib_fl)\n",
    "print(f'Started Extracting GE tar file to {LOCAL_TARFile_DIR} ...')\n",
    "file.extractall(f'{LOCAL_LIB_DIR}/ge')\n",
    "file.close()\n",
    "print(f'Done extracting GE tar file to {LOCAL_TARFile_DIR} ...')\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d673dfa-e374-4ac4-b6ee-0dbcbaf36fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./temp/libs/ge/great_expectations-0.15.14/great_expectations\n"
     ]
    }
   ],
   "source": [
    "# Getting the path for the great_expectation folder after the tar file is extracted. This path is used in the session.add_imports()\n",
    "\n",
    "import glob\n",
    "ge_import_path=''\n",
    "for result in glob.iglob('./temp/libs/ge/great_expectations*'):\n",
    "    ge_import_path=result+'/great_expectations'\n",
    "print(ge_import_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a03bd0-f804-4e82-a531-63aa8b50eecf",
   "metadata": {},
   "source": [
    "### Creating Python Stored Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "caac0c0e-f54f-43b8-ae50-1022e2499b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "from great_expectations.data_context.types.base import DataContextConfig, DatasourceConfig, S3StoreBackendDefaults\n",
    "from config import snowflake_conn_prop_local as snowflake_udf_conn_prop\n",
    "from great_expectations.core.batch import BatchRequest, RuntimeBatchRequest\n",
    "from great_expectations.data_context import BaseDataContext\n",
    "from snowflake.snowpark.types import IntegerType, StringType, StructField,VariantType,StructType,BooleanType\n",
    "from great_expectations.checkpoint.types.checkpoint_result import CheckpointResult\n",
    "from great_expectations.data_context import DataContext\n",
    "from config import snowflake_conn_prop_local as snowflake_conn_prop\n",
    "from great_expectations.data_context import BaseDataContext\n",
    "from great_expectations.data_context.types.base import DataContextConfig, DatasourceConfig, FilesystemStoreBackendDefaults\n",
    "from great_expectations.checkpoint import Checkpoint\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "session.sql(\"create or replace stage phani_greatexpectation\").collect()\n",
    "session.clear_packages()\n",
    "session.add_packages('pandas','pycryptodomex','boto3','tzlocal','tqdm','requests','ruamel.yaml','ipython','jsonpatch','mistune','jinja2','jsonschema','scipy','altair','Click','colorama','cryptography','snowflake-snowpark-python','sqlalchemy','chardet','asn1crypto')\n",
    "session.clear_imports()\n",
    "session.add_import(ge_import_path)\n",
    "# session.add_import('jwt')\n",
    "\n",
    "@sproc(session=session,name=\"usp_generateValidationResults\", replace=True, return_type=StringType(), is_permanent=True, stage_location='@phani_greatexpectation/ge_AllLibs')\n",
    "def generateValidationResults(session: Session) -> str:\n",
    "    \n",
    "    from pathlib import Path\n",
    "    import os ,sys ,json ,tarfile\n",
    "    \n",
    "    data_context_config = DataContextConfig(\n",
    "    datasources={\n",
    "        \"dataframe_datasource\": DatasourceConfig(\n",
    "            class_name=\"PandasDatasource\",\n",
    "            batch_kwargs_generators={\n",
    "                \"subdir_reader\": {\n",
    "                    \"class_name\": \"SubdirReaderBatchKwargsGenerator\",\n",
    "                    \"base_directory\": \"/tmp/great_expectation/\",\n",
    "                }\n",
    "            },\n",
    "        )\n",
    "    },\n",
    "    store_backend_defaults=FilesystemStoreBackendDefaults(root_directory=\"/tmp/great_expectation\"),\n",
    "    )\n",
    "    \n",
    "    # Creating the GE context here\n",
    "    context = BaseDataContext(project_config=data_context_config)\n",
    "    \n",
    "    # Providing the datasource details which here is the pandas DF. We define the actual DF in the batch request which is defined after creating the DS\n",
    "    \n",
    "    datasource_config = {\n",
    "    \"name\": \"pandas_dataframe_datasource\",\n",
    "    \"class_name\": \"Datasource\",\n",
    "    \"module_name\": \"great_expectations.datasource\",\n",
    "    \"execution_engine\": {\n",
    "        \"module_name\": \"great_expectations.execution_engine\",\n",
    "        \"class_name\": \"PandasExecutionEngine\",\n",
    "    },\n",
    "    \"data_connectors\": {\n",
    "        \"default_runtime_data_connector_name\": {\n",
    "            \"class_name\": \"RuntimeDataConnector\",\n",
    "            \"module_name\": \"great_expectations.datasource.data_connector\",\n",
    "            \"batch_identifiers\": [\"default_identifier_name\"],\n",
    "        },\n",
    "    },\n",
    "            }\n",
    "    con='done'\n",
    "\n",
    "    # Adding the DS to the context\n",
    "    context.add_datasource(**datasource_config)\n",
    "    \n",
    "    # Converting the Snowpark DF into Pandas DF.\n",
    "    df=session.sql(\"select top 2000 * from TAXI_TRIPS_MAT_VIEW\").to_pandas()\n",
    "    \n",
    "    #Creating the batch request whivh will be used \n",
    "    batch_request = RuntimeBatchRequest(\n",
    "                                datasource_name=\"pandas_dataframe_datasource\",\n",
    "                                data_connector_name=\"default_runtime_data_connector_name\",\n",
    "                                data_asset_name=\"PandasData\",  # This can be anything that identifies this data_asset for you\n",
    "                                runtime_parameters={\"batch_data\": df},  # df is your dataframe, you have created above.\n",
    "                                batch_identifiers={\"default_identifier_name\": \"default_identifier\"},\n",
    "                                )\n",
    "    \n",
    "    # Creating the expecation suite\n",
    "    context.create_expectation_suite(\n",
    "    expectation_suite_name=\"pandas_expectation_suite\", overwrite_existing=True)\n",
    "    \n",
    "    # Creating the validator which takes the batch request and expectation suite name\n",
    "    validator = context.get_validator(\n",
    "        batch_request=batch_request, expectation_suite_name=\"pandas_expectation_suite\"\n",
    "    )\n",
    "    \n",
    "    #Creating the required expectation. You can also create custom expectations as well. You can add additional inbuilt expectations as per the requirement\n",
    "    validator.expect_column_values_to_be_in_set(\"TAXI_TYPE\",[\"yellow\",\"green\"])\n",
    "    validator.expect_column_min_to_be_between(\"VENDOR_ID\",1,10)\n",
    "    validator.expect_column_mean_to_be_between(\"VENDOR_ID\",2,14)\n",
    "    \n",
    "    #Saving the expectation \n",
    "    validator.save_expectation_suite(discard_failed_expectations=False)\n",
    "    \n",
    "    # Creating the checkpoint without writing to the file system and running by passing the run time parameters\n",
    "\n",
    "    my_checkpoint_name = \"pandas_checkpoint\"\n",
    "    checkpoint_config = {\n",
    "                \"name\": my_checkpoint_name,\n",
    "                \"config_version\": 1.0,\n",
    "                \"class_name\": \"SimpleCheckpoint\",\n",
    "                \"run_name_template\": \"%Y%m%d-%H%M%S-my-pandas_run-name-template\",\n",
    "            }\n",
    "            \n",
    "    context.add_checkpoint(**checkpoint_config)\n",
    "\n",
    "    # run expectation_suite against Pandas dataframe\n",
    "    res = context.run_checkpoint(\n",
    "            checkpoint_name = my_checkpoint_name,\n",
    "            validations=[\n",
    "                {\n",
    "                    \"batch_request\": batch_request,\n",
    "                    \"expectation_suite_name\": \"pandas_expectation_suite\",\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "    \n",
    "        \n",
    "    # Defining the schema, creating the Snowpark DF and and writing the validation results to a table.\n",
    "    schema = StructType([StructField(\"RunStatus\", BooleanType()),StructField(\"RunId\", VariantType()), StructField(\"RunValidation\", VariantType())])\n",
    "\n",
    "    df=session.create_dataframe([[res.success, json.loads(str(res.run_id)),json.loads(str(res.list_validation_results()))]], schema)\n",
    "\n",
    "    df.write.mode('append').saveAsTable('GreatExpeactionValidationsResults')\n",
    "    return 'SUCCESS'\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e3abb2f-dda0-4bae-93fa-60bcb7539abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(USP_GENERATEVALIDATIONRESULTS='SUCCESS')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sql(\"call usp_generateValidationResults()\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848e67d5-24c5-4f7c-9cfb-c62345dd5736",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
